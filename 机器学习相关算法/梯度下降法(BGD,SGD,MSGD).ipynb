{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降法(BGD,SGD,MSGD)python+numpy具体实现 参考链接: https://blog.csdn.net/pengjian444/article/details/71075544\n",
    "# 机器学习中常见的最优化算法 参考链接: https://blog.csdn.net/wtq1993/article/details/51607040\n",
    "# f(x,y)=(1−x)^2+100(y−x^2)^2  目标:求解最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #全部行都能输出\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rosenbrock(x1=0.0, x2=0.0):\n",
    "    \"\"\"\n",
    "    计算rosenbrock函数的值\n",
    "    :param x1:\n",
    "    :param x2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return (1 - x1) ** 2 + 100 * (x2 - x1 ** 2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降方法求解最小值\n",
    "def cal_partial_x(x1=0.0, x2=0.0):\n",
    "    \"\"\"\n",
    "    对x求偏导\n",
    "    \"\"\"\n",
    "    return 2 * (x1 - 1) + 400 * x1 * (x1 ** 2 - x2)\n",
    "\n",
    "def cal_partial_y(x1, x2):\n",
    "    \"\"\"\n",
    "    对x求偏导\n",
    "    \"\"\"\n",
    "    return 200 * (x2 - x1 ** 2)\n",
    "\n",
    "\n",
    "def gd_for_min(max_iter_count=100000, step_size=0.001):\n",
    "    \"\"\"\n",
    "    梯度下降寻找最小值\n",
    "    \"\"\"\n",
    "    # 初始化 x,y的值\n",
    "    par_x = np.zeros((2,), dtype=np.float32)\n",
    "    # 损失值\n",
    "    loss = 10\n",
    "    num_iter = 0\n",
    "    while loss > 0.001 and num_iter < max_iter_count:\n",
    "        # 如果损失大于0.001 并且迭代次数小于max_iter_count\n",
    "        error = np.zeros((2,), dtype=np.float32)\n",
    "        error[0] = cal_partial_x(par_x[0], par_x[1])\n",
    "        error[1] = cal_partial_y(par_x[0], par_x[1])\n",
    "        \n",
    "        for i in range(len(par_x)):\n",
    "            # 沿梯度的反方向更新par_x\n",
    "            par_x[i] -= step_size * error[i]\n",
    "        # 目标函数的最小值为0  所以带入原函数这个就是损失\n",
    "        loss = cal_rosenbrock(par_x[0], par_x[1])\n",
    "        if num_iter % 50 == 0:\n",
    "            print(\"iter_count: \", num_iter, \"the loss:\", loss)\n",
    "        num_iter += 1\n",
    "    return par_x\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  0 the loss: 0.9960040014103906\n",
      "iter_count:  50 the loss: 0.8173250023078208\n",
      "iter_count:  100 the loss: 0.6787215496065403\n",
      "iter_count:  150 the loss: 0.572809407603099\n",
      "iter_count:  200 the loss: 0.49101944184659146\n",
      "iter_count:  250 the loss: 0.4266009306452159\n",
      "iter_count:  300 the loss: 0.3748012101126237\n",
      "iter_count:  350 the loss: 0.33235324151747075\n",
      "iter_count:  400 the loss: 0.29699262257045417\n",
      "iter_count:  450 the loss: 0.2671196352393164\n",
      "iter_count:  500 the loss: 0.2415783819389332\n",
      "iter_count:  550 the loss: 0.21951566998896602\n",
      "iter_count:  600 the loss: 0.20028837472778116\n",
      "iter_count:  650 the loss: 0.18340317654635094\n",
      "iter_count:  700 the loss: 0.1684749770814551\n",
      "iter_count:  750 the loss: 0.15519886682400053\n",
      "iter_count:  800 the loss: 0.14333008982436451\n",
      "iter_count:  850 the loss: 0.13266987046080342\n",
      "iter_count:  900 the loss: 0.12305516334901166\n",
      "iter_count:  950 the loss: 0.11435083375122591\n",
      "iter_count:  1000 the loss: 0.10644370829015032\n",
      "iter_count:  1050 the loss: 0.09923869433734246\n",
      "iter_count:  1100 the loss: 0.09265469064294508\n",
      "iter_count:  1150 the loss: 0.08662279313952723\n",
      "iter_count:  1200 the loss: 0.08108353954112288\n",
      "iter_count:  1250 the loss: 0.0759854868832288\n",
      "iter_count:  1300 the loss: 0.07128403579224489\n",
      "iter_count:  1350 the loss: 0.06694005521812525\n",
      "iter_count:  1400 the loss: 0.06291944761790248\n",
      "iter_count:  1450 the loss: 0.05919212643218744\n",
      "iter_count:  1500 the loss: 0.055731343726218116\n",
      "iter_count:  1550 the loss: 0.05251357196851352\n",
      "iter_count:  1600 the loss: 0.04951767382836191\n",
      "iter_count:  1650 the loss: 0.046724889830650526\n",
      "iter_count:  1700 the loss: 0.04411833598466562\n",
      "iter_count:  1750 the loss: 0.041682994606636346\n",
      "iter_count:  1800 the loss: 0.03940507662477746\n",
      "iter_count:  1850 the loss: 0.03727237849409177\n",
      "iter_count:  1900 the loss: 0.035273788912412035\n",
      "iter_count:  1950 the loss: 0.0333992315347312\n",
      "iter_count:  2000 the loss: 0.031639440467983594\n",
      "iter_count:  2050 the loss: 0.029986125482156824\n",
      "iter_count:  2100 the loss: 0.02843158765716284\n",
      "iter_count:  2150 the loss: 0.02696897376636656\n",
      "iter_count:  2200 the loss: 0.02559178309338721\n",
      "iter_count:  2250 the loss: 0.02429421200376132\n",
      "iter_count:  2300 the loss: 0.023070897752485692\n",
      "iter_count:  2350 the loss: 0.0219169051515402\n",
      "iter_count:  2400 the loss: 0.02082767047852789\n",
      "iter_count:  2450 the loss: 0.019798973052612596\n",
      "iter_count:  2500 the loss: 0.018826927488699293\n",
      "iter_count:  2550 the loss: 0.01790797397297473\n",
      "iter_count:  2600 the loss: 0.01703880724217393\n",
      "iter_count:  2650 the loss: 0.01621635694544965\n",
      "iter_count:  2700 the loss: 0.015437673855279629\n",
      "iter_count:  2750 the loss: 0.014700148455625957\n",
      "iter_count:  2800 the loss: 0.014001342306469282\n",
      "iter_count:  2850 the loss: 0.013338943765619511\n",
      "iter_count:  2900 the loss: 0.012710808930662258\n",
      "iter_count:  2950 the loss: 0.012114951820099307\n",
      "iter_count:  3000 the loss: 0.011549505451045189\n",
      "iter_count:  3050 the loss: 0.011012751159605413\n",
      "iter_count:  3100 the loss: 0.010503061697872358\n",
      "iter_count:  3150 the loss: 0.010018894037917829\n",
      "iter_count:  3200 the loss: 0.009558856982582813\n",
      "iter_count:  3250 the loss: 0.009121601921732565\n",
      "iter_count:  3300 the loss: 0.008705892623207846\n",
      "iter_count:  3350 the loss: 0.008310540010906566\n",
      "iter_count:  3400 the loss: 0.007934457573411614\n",
      "iter_count:  3450 the loss: 0.007576607606444271\n",
      "iter_count:  3500 the loss: 0.0072360216528549745\n",
      "iter_count:  3550 the loss: 0.006911791389394937\n",
      "iter_count:  3600 the loss: 0.006603041962655926\n",
      "iter_count:  3650 the loss: 0.006308997849417948\n",
      "iter_count:  3700 the loss: 0.0060288643909889635\n",
      "iter_count:  3750 the loss: 0.0057619388745676035\n",
      "iter_count:  3800 the loss: 0.005507537321856414\n",
      "iter_count:  3850 the loss: 0.005265046870143709\n",
      "iter_count:  3900 the loss: 0.005033829209120781\n",
      "iter_count:  3950 the loss: 0.0048133546115313895\n",
      "iter_count:  4000 the loss: 0.004603065469277591\n",
      "iter_count:  4050 the loss: 0.0044024525495986785\n",
      "iter_count:  4100 the loss: 0.004211053235318301\n",
      "iter_count:  4150 the loss: 0.0040284030132138274\n",
      "iter_count:  4200 the loss: 0.0038540639999944926\n",
      "iter_count:  4250 the loss: 0.0036876534128505675\n",
      "iter_count:  4300 the loss: 0.0035287676083164254\n",
      "iter_count:  4350 the loss: 0.0033770482581058346\n",
      "iter_count:  4400 the loss: 0.0032321539496508226\n",
      "iter_count:  4450 the loss: 0.003093750229298392\n",
      "iter_count:  4500 the loss: 0.0029615379384652476\n",
      "iter_count:  4550 the loss: 0.002835212120904175\n",
      "iter_count:  4600 the loss: 0.0027144985187027466\n",
      "iter_count:  4650 the loss: 0.0025991472805644423\n",
      "iter_count:  4700 the loss: 0.00248888159621731\n",
      "iter_count:  4750 the loss: 0.0023834909247331475\n",
      "iter_count:  4800 the loss: 0.0022827276673260755\n",
      "iter_count:  4850 the loss: 0.0021863869474630627\n",
      "iter_count:  4900 the loss: 0.002094251776593275\n",
      "iter_count:  4950 the loss: 0.0020061481180682268\n",
      "iter_count:  5000 the loss: 0.0019218857157053215\n",
      "iter_count:  5050 the loss: 0.00184128034579501\n",
      "iter_count:  5100 the loss: 0.0017641672978060006\n",
      "iter_count:  5150 the loss: 0.0016903927319923155\n",
      "iter_count:  5200 the loss: 0.0016197949740563555\n",
      "iter_count:  5250 the loss: 0.0015522458823852415\n",
      "iter_count:  5300 the loss: 0.0014876055742240421\n",
      "iter_count:  5350 the loss: 0.0014257308217397707\n",
      "iter_count:  5400 the loss: 0.0013665075727464842\n",
      "iter_count:  5450 the loss: 0.001309817037751915\n",
      "iter_count:  5500 the loss: 0.001255544376248503\n",
      "iter_count:  5550 the loss: 0.0012035863595502606\n",
      "iter_count:  5600 the loss: 0.0011538329084188192\n",
      "iter_count:  5650 the loss: 0.001106188414640239\n",
      "iter_count:  5700 the loss: 0.0010605603100074322\n",
      "iter_count:  5750 the loss: 0.001016865103705447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.96840495, 0.9376793 ], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# array([0.96840495, 0.9376793 ], dtype=float32)\n",
    "gd_for_min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BGD（批量梯度下降法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_line_data(sample_num=100):\n",
    "    \"\"\"\n",
    "    y = 3*x1 + 4*x2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(0, 9, sample_num)\n",
    "    x2 = np.linspace(4, 13, sample_num)\n",
    "    # x1, x2 搞成两列 相当于特征列 concatenate和vstack作用是一样的\n",
    "    x = np.concatenate(([x1], [x2]), axis=0).T\n",
    "    y = np.dot(x, np.array([3, 4]).T)  # y 列向量\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  8],\n",
       "        [ 1,  9],\n",
       "        [ 2, 10],\n",
       "        [ 3, 11]],\n",
       "\n",
       "       [[ 4, 12],\n",
       "        [ 5, 13],\n",
       "        [ 6, 14],\n",
       "        [ 7, 15]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 8,  9, 10, 11]],\n",
       "\n",
       "       [[ 4,  5,  6,  7],\n",
       "        [12, 13, 14, 15]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array(range(8)).reshape(2, 4)\n",
    "x2 = np.array(range(8, 16)).reshape(2, 4)\n",
    "np.vstack((x1, x2))\n",
    "np.stack([x1, x2], axis=-1)\n",
    "np.stack([x1, x2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>4.090909</td>\n",
       "      <td>16.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>17.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>4.272727</td>\n",
       "      <td>17.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>4.363636</td>\n",
       "      <td>18.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>4.454545</td>\n",
       "      <td>19.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>19.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>20.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>4.727273</td>\n",
       "      <td>21.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>4.818182</td>\n",
       "      <td>21.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1          y\n",
       "0  0.000000  4.000000  16.000000\n",
       "1  0.090909  4.090909  16.636364\n",
       "2  0.181818  4.181818  17.272727\n",
       "3  0.272727  4.272727  17.909091\n",
       "4  0.363636  4.363636  18.545455\n",
       "5  0.454545  4.454545  19.181818\n",
       "6  0.545455  4.545455  19.818182\n",
       "7  0.636364  4.636364  20.454545\n",
       "8  0.727273  4.727273  21.090909\n",
       "9  0.818182  4.818182  21.727273"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = gen_line_data()\n",
    "df = pd.DataFrame(x)\n",
    "df['y'] = y\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归的推导\n",
    "![image](img/1.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgd_for_min(samples, y, step_size=0.01, max_iter_count=10000):\n",
    "    \"\"\"\n",
    "    批量梯度下降求sita (在demo中是w)\n",
    "    \"\"\"\n",
    "    sample_num, dim = samples.shape\n",
    "    w = np.zeros((dim,), dtype=np.float32)\n",
    "    num_iter = 0\n",
    "    loss = 10\n",
    "    while loss > 0.001 and num_iter < max_iter_count:\n",
    "        # 这个不能叫错误 准确来说是梯度向量\n",
    "#         error = np.zeros((dim, ), dtype=np.float32)\n",
    "        gradient = np.zeros((dim, ), dtype=np.float32)\n",
    "\n",
    "        loss = 0\n",
    "        # 遍历样本\n",
    "        for i in range(sample_num):\n",
    "            pred_y = np.dot(w, samples[i])\n",
    "            for j in range(dim):\n",
    "                gradient[j] += (y[i] - pred_y) * samples[i, j]\n",
    "                \n",
    "        for j in range(dim):\n",
    "            w[j] += step_size * gradient[j] / sample_num\n",
    "            \n",
    "        for i in range(sample_num):\n",
    "            pred_y = np.dot(w, samples[i])\n",
    "            loss += (y[i] - pred_y) ** 2 \n",
    "        loss = loss / sample_num\n",
    "        if num_iter % 50 == 0:\n",
    "            print(\"num_iter: \", num_iter, \"the loss:\", loss)\n",
    "        num_iter += 1\n",
    "    return w    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iter:  0 the loss: 7.445309030387418\n",
      "num_iter:  50 the loss: 0.12695212622054702\n",
      "num_iter:  100 the loss: 0.044309058318090594\n",
      "num_iter:  150 the loss: 0.015464655396450406\n",
      "num_iter:  200 the loss: 0.0053973412421002755\n",
      "num_iter:  250 the loss: 0.0018838139222933526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.9735017, 4.015316 ], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgd_for_min(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGB（随机梯度下降法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造线性回归数据\n",
    "def gen_line_data(sample_num=100):\n",
    "    \"\"\"\n",
    "    y = 3*x1 + 4*x2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(0, 9, sample_num)\n",
    "    x2 = np.linspace(4, 13, sample_num)\n",
    "    x = np.concatenate(([x1], [x2]), axis=0).T\n",
    "    y = np.dot(x, np.array([3, 4]).T)  # y 列向量\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_line_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归的推导\n",
    "![image](img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_for_min(samples, y, step_size=0.01, max_iter_count=10000):\n",
    "    \"\"\"\n",
    "    随机梯度下降求参数\n",
    "    \"\"\"\n",
    "    sample_num, dim = samples.shape\n",
    "    loss = 10\n",
    "    num_iter = 0\n",
    "    w = np.zeros((dim, ), dtype=np.float32)\n",
    "    while loss > 0.001 and num_iter < max_iter_count:\n",
    "        loss = 0\n",
    "        gradient = np.zeros((dim, ), dtype=np.float32)\n",
    "        for i in range(sample_num):\n",
    "            pred_y = np.dot(w, samples[i])\n",
    "            for j in range(dim):\n",
    "                gradient[j] += (y[i] - pred_y) * samples[i, j]\n",
    "                w[j] += step_size * gradient[j] /sample_num\n",
    "        for i in range(sample_num):\n",
    "            pred_y = np.dot(w, samples[i])\n",
    "            loss += (y[i] - pred_y) ** 2\n",
    "        loss = loss / sample_num\n",
    "#         if num_iter % 50 == 0:\n",
    "#             print(\"num_iter: \", num_iter, \"the loss:\", loss)\n",
    "        print(\"num_iter: \", num_iter, \"the loss:\", loss)\n",
    "\n",
    "        num_iter += 1\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iter:  0 the loss: 899.5256395123043\n",
      "num_iter:  1 the loss: 429.3892866487446\n",
      "num_iter:  2 the loss: 139.34765711265553\n",
      "num_iter:  3 the loss: 73.24721571797343\n",
      "num_iter:  4 the loss: 21.75037765840998\n",
      "num_iter:  5 the loss: 13.14228278589012\n",
      "num_iter:  6 the loss: 3.530455803426953\n",
      "num_iter:  7 the loss: 2.555822593919807\n",
      "num_iter:  8 the loss: 0.6408691516613793\n",
      "num_iter:  9 the loss: 0.5572900621999998\n",
      "num_iter:  10 the loss: 0.1440751635975759\n",
      "num_iter:  11 the loss: 0.1391997526762292\n",
      "num_iter:  12 the loss: 0.041638116822024465\n",
      "num_iter:  13 the loss: 0.039598945694037185\n",
      "num_iter:  14 the loss: 0.014328956046269192\n",
      "num_iter:  15 the loss: 0.012468114931806409\n",
      "num_iter:  16 the loss: 0.005328085480538321\n",
      "num_iter:  17 the loss: 0.004200748110832823\n",
      "num_iter:  18 the loss: 0.002026801292554504\n",
      "num_iter:  19 the loss: 0.0014756904896971385\n",
      "num_iter:  20 the loss: 0.0007728594738228855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.9770935, 4.012334 ], dtype=float32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_for_min(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MBGD(小批量梯度下降法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造线性数据\n",
    "def gen_line_data(sample_num=100):\n",
    "    \"\"\"\n",
    "    y = 3*x1 + 4*x2\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(0, 9, sample_num)\n",
    "    x2 = np.linspace(4, 13, sample_num)\n",
    "    x = np.concatenate(([x1], [x2]), axis=0).T\n",
    "    y = np.dot(x, np.array([3, 4]).T)  # y 列向量\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = gen_line_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbgd_for_min(samples, y, step_size=0.01, max_iter_count=10000, batch_size=0.2):\n",
    "    \"\"\"\n",
    "    小批量梯度下降\n",
    "    \"\"\"\n",
    "    sample_num, dim = samples.shape\n",
    "    loss = 10\n",
    "    num_iter = 0\n",
    "    w = np.zeros((dim, ), dtype=np.float32)\n",
    "    while loss > 0.001 and num_iter < max_iter_count:\n",
    "        loss = 0\n",
    "        # 每个特征对应的梯度\n",
    "        gradient = np.zeros((dim, ), dtype=np.float32)\n",
    "        # 每次从samples中随机抽取int(np.ceil(sample_num * batch_size))个数据, 返回索引数组\n",
    "        sub_samples_index = random.sample(range(sample_num), int(np.ceil(sample_num * batch_size)))\n",
    "        \n",
    "        batch_samples = samples[sub_samples_index]\n",
    "        batch_y = y[sub_samples_index]\n",
    "        \n",
    "        for i in range(len(batch_samples)):\n",
    "            pred_y = np.dot(w, batch_samples[i])\n",
    "            # 计算每个特征对应的梯度\n",
    "            for j in range(dim):\n",
    "                gradient[j] += (batch_y[i] - pred_y) * batch_samples[i, j]\n",
    "        # 更新w, \n",
    "        for j in range(dim):\n",
    "            w[j] += step_size * gradient[j] / sample_num\n",
    "            \n",
    "        for i in range(sample_num):\n",
    "            pred_y = np.dot(w, samples[i])\n",
    "            loss += (y[i] - pred_y) ** 2\n",
    "        loss = loss / sample_num\n",
    "        if num_iter % 50 == 0:\n",
    "            print(\"num_iter: \", num_iter, \"the loss:\", loss)\n",
    "        num_iter += 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iter:  0 the loss: 1713.6820139059557\n",
      "num_iter:  50 the loss: 0.2799037709046905\n",
      "num_iter:  100 the loss: 0.23327767956244438\n",
      "num_iter:  150 the loss: 0.18710766353661834\n",
      "num_iter:  200 the loss: 0.15202963556430413\n",
      "num_iter:  250 the loss: 0.1235119045090971\n",
      "num_iter:  300 the loss: 0.10079747185001126\n",
      "num_iter:  350 the loss: 0.08288594255328591\n",
      "num_iter:  400 the loss: 0.06758127407031435\n",
      "num_iter:  450 the loss: 0.05537068611119686\n",
      "num_iter:  500 the loss: 0.04411088113527686\n",
      "num_iter:  550 the loss: 0.03561444898750987\n",
      "num_iter:  600 the loss: 0.028728431039987637\n",
      "num_iter:  650 the loss: 0.023555662064661052\n",
      "num_iter:  700 the loss: 0.018925377442524344\n",
      "num_iter:  750 the loss: 0.015234254486513651\n",
      "num_iter:  800 the loss: 0.012075845585857548\n",
      "num_iter:  850 the loss: 0.009743275067814278\n",
      "num_iter:  900 the loss: 0.007884798577751172\n",
      "num_iter:  950 the loss: 0.006361021247143269\n",
      "num_iter:  1000 the loss: 0.005259168491399806\n",
      "num_iter:  1050 the loss: 0.004231007863105341\n",
      "num_iter:  1100 the loss: 0.0034617734577295726\n",
      "num_iter:  1150 the loss: 0.0028230764825784944\n",
      "num_iter:  1200 the loss: 0.0022864562129246745\n",
      "num_iter:  1250 the loss: 0.0018630381428527839\n",
      "num_iter:  1300 the loss: 0.0015144117846959467\n",
      "num_iter:  1350 the loss: 0.00123279312998132\n",
      "num_iter:  1400 the loss: 0.0010009608396563323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.9732964, 4.0154138], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbgd_for_min(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
