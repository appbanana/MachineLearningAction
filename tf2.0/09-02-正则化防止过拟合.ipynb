{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd 先进入当前路径\n",
    "# tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置项\n",
    "# # 这个要放到设置中文之前否则还是小方框\n",
    "# plt.style.use(\"seaborn\")\n",
    "\n",
    "# # 指定默认字体 用来正常显示中文标签\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# # 解决保存图像是负号'-'显示为方块的问题\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# #全部行都能输出\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.map(preprocess_data).shuffle(10000).batch(128).repeat(10)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess_data).shuffle(10000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  330       \n",
      "=================================================================\n",
      "Total params: 576,810\n",
      "Trainable params: 576,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.build(input_shape=[None, 28 * 28])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输出日志 用于可视化\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = 'logs/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(logdir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一种 手动计算l1范数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.2199015617370605 loss_regularization: 405.07244873046875\n",
      "【正确率 accuracy_metrics = 0.487200】\n",
      "100 loss: 0.593031108379364 loss_regularization: 253.77618408203125\n",
      "200 loss: 0.577171266078949 loss_regularization: 262.68182373046875\n",
      "300 loss: 0.4717406928539276 loss_regularization: 258.01959228515625\n",
      "400 loss: 0.6325197815895081 loss_regularization: 247.8853302001953\n",
      "500 loss: 0.4656405746936798 loss_regularization: 270.90911865234375\n",
      "【正确率 accuracy_metrics = 0.932900】\n",
      "600 loss: 0.3572179675102234 loss_regularization: 261.4109802246094\n",
      "700 loss: 0.5480451583862305 loss_regularization: 256.2273864746094\n",
      "800 loss: 0.3889096975326538 loss_regularization: 229.26123046875\n",
      "900 loss: 0.475649356842041 loss_regularization: 250.99388122558594\n",
      "1000 loss: 0.3884015679359436 loss_regularization: 260.1014099121094\n",
      "【正确率 accuracy_metrics = 0.948100】\n",
      "1100 loss: 0.5256780385971069 loss_regularization: 273.0445556640625\n",
      "1200 loss: 0.3482818901538849 loss_regularization: 249.66993713378906\n",
      "1300 loss: 0.5145895481109619 loss_regularization: 253.18878173828125\n",
      "1400 loss: 0.41442662477493286 loss_regularization: 231.9402618408203\n",
      "1500 loss: 0.5769621729850769 loss_regularization: 256.32916259765625\n",
      "【正确率 accuracy_metrics = 0.935900】\n",
      "1600 loss: 0.40773510932922363 loss_regularization: 255.65074157714844\n",
      "1700 loss: 0.5769681930541992 loss_regularization: 305.7776794433594\n",
      "1800 loss: 0.46392279863357544 loss_regularization: 258.7995300292969\n",
      "1900 loss: 0.4124101996421814 loss_regularization: 217.3896026611328\n",
      "2000 loss: 0.3603011965751648 loss_regularization: 237.66104125976562\n",
      "【正确率 accuracy_metrics = 0.956000】\n",
      "2100 loss: 0.46122485399246216 loss_regularization: 244.58314514160156\n",
      "2200 loss: 0.4227781891822815 loss_regularization: 244.7624969482422\n",
      "2300 loss: 0.4368044137954712 loss_regularization: 227.01116943359375\n",
      "2400 loss: 0.38674622774124146 loss_regularization: 218.42909240722656\n",
      "2500 loss: 0.39465171098709106 loss_regularization: 225.3773193359375\n",
      "【正确率 accuracy_metrics = 0.952200】\n",
      "2600 loss: 0.5257791876792908 loss_regularization: 236.47653198242188\n",
      "2700 loss: 0.4122333824634552 loss_regularization: 273.00091552734375\n",
      "2800 loss: 0.4828605055809021 loss_regularization: 236.0699920654297\n",
      "2900 loss: 0.4367057979106903 loss_regularization: 241.69207763671875\n",
      "3000 loss: 0.46218395233154297 loss_regularization: 240.6050567626953\n",
      "【正确率 accuracy_metrics = 0.952300】\n",
      "3100 loss: 0.48276403546333313 loss_regularization: 267.5958557128906\n",
      "3200 loss: 0.41477006673812866 loss_regularization: 284.76214599609375\n",
      "3300 loss: 0.35148885846138 loss_regularization: 226.1904296875\n",
      "3400 loss: 0.47032105922698975 loss_regularization: 225.1014404296875\n",
      "3500 loss: 0.38889575004577637 loss_regularization: 252.77020263671875\n",
      "【正确率 accuracy_metrics = 0.954900】\n",
      "3600 loss: 0.4814622402191162 loss_regularization: 250.5238800048828\n",
      "3700 loss: 0.2999911308288574 loss_regularization: 225.86013793945312\n",
      "3800 loss: 0.364035427570343 loss_regularization: 238.96730041503906\n",
      "3900 loss: 0.33882367610931396 loss_regularization: 247.1797637939453\n",
      "4000 loss: 0.4598303735256195 loss_regularization: 241.86822509765625\n",
      "【正确率 accuracy_metrics = 0.949900】\n",
      "4100 loss: 0.41317397356033325 loss_regularization: 251.789306640625\n",
      "4200 loss: 0.33404889702796936 loss_regularization: 220.87554931640625\n",
      "4300 loss: 0.3884091377258301 loss_regularization: 235.9573974609375\n",
      "4400 loss: 0.5505185723304749 loss_regularization: 266.03363037109375\n",
      "4500 loss: 0.40371328592300415 loss_regularization: 250.6817169189453\n",
      "【正确率 accuracy_metrics = 0.955800】\n",
      "4600 loss: 0.40298783779144287 loss_regularization: 237.3795928955078\n"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "optimizer = keras.optimizers.Adam(0.01)\n",
    "\n",
    "# 统计准确率\n",
    "accuracy_metrics = keras.metrics.Accuracy()\n",
    "\n",
    "# 开始训练\n",
    "for step, (x, y) in enumerate(db_train):\n",
    "    x = tf.reshape(x, [-1, 28 * 28])\n",
    "    with tf.GradientTape() as tap:\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        y = tf.one_hot(y, depth=10)\n",
    "        y_pred = model(x)\n",
    "        loss = keras.losses.categorical_crossentropy(y, y_pred, from_logits=True)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # 正则化\n",
    "        loss_regularization = []\n",
    "        for p in model.trainable_variables:\n",
    "            loss_regularization.append(tf.nn.l2_loss(p))\n",
    "         \n",
    "        # l2正则化 左右参数相加求和\n",
    "        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n",
    "        \n",
    "        loss = loss + 0.001 * loss_regularization\n",
    "    \n",
    "    # 对参数球梯度 更新参数\n",
    "    grads = tap.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        accuracy_metrics.reset_states()\n",
    "        \n",
    "        for (x_, y_) in db_test:\n",
    "            x_ = tf.reshape(x_, [-1, 28*28])\n",
    "            y_pred = model(x_)\n",
    "            y_pred = tf.argmax(y_pred, axis=1, output_type=tf.int32)\n",
    "            \n",
    "            accuracy_metrics.update_state(y_, y_pred)\n",
    "            \n",
    "        print('【正确率 accuracy_metrics = %f】' % (accuracy_metrics.result().numpy()))    \n",
    " \n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('test_accuracy', accuracy_metrics.result().numpy(), step=step)\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二种直接设置进去\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 数据预处理\n",
    "# def preprocess_data2(x, y):\n",
    "#     x = tf.cast(x, dtype=tf.float32) / 255\n",
    "#     x = tf.reshape(x, [28 * 28])\n",
    "#     y = tf.cast(y, dtype=tf.int32)\n",
    "#     y = tf.one_hot(y, depth=10)\n",
    "#     return x, y\n",
    "\n",
    "\n",
    "# # 划分训练接和测试集\n",
    "batch_size = 128\n",
    "db_train_ = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train_ = db_train.map(preprocess_data).shuffle(10000).batch(batch_size)\n",
    "\n",
    "db_test_ = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test_ = db_test.map(preprocess_data).batch(batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(512, kernel_regularizer=keras.regularizers.l2(0.001), activation=tf.nn.relu, input_shape=(28*28, )),\n",
    "    keras.layers.Dense(128, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'),\n",
    "    keras.layers.Dense(32, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "# model2.compile(optimizer='adam', \n",
    "#                loss=tf.losses.CategoricalCrossentropy(from_logits=True), \n",
    "#                metrics=['accuracy'])\n",
    "\n",
    "model2.build(input_shape=[None, 28 * 28])\n",
    "\n",
    "model2.compile(optimizer=keras.optimizers.Adam(1e-3), \n",
    "              loss=tf.losses.CategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_38_input to have 2 dimensions, but got array with shape (60000, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-fd361628e270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#           )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2426\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2428\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    510\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_38_input to have 2 dimensions, but got array with shape (60000, 28, 28)"
     ]
    }
   ],
   "source": [
    "# model2.fit(db_train, epochs=10, validation_data=db_test, validation_freq=2)\n",
    "\n",
    "# model2.fit(db_train, \n",
    "#           epochs=10,  # 训练次数\n",
    "#           validation_freq=1, # 验证集调用的评率\n",
    "#           validation_data=db_test\n",
    "#           )\n",
    "\n",
    "model2.fit(x_train, y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bc6cb55a1b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'db_train' is not defined"
     ]
    }
   ],
   "source": [
    "db_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
