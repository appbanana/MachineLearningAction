{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(imgs, name):\n",
    "    \"\"\"\n",
    "    多张小图合成一张大图\n",
    "    \"\"\"\n",
    "    new_im = Image.new('L', (280, 280))\n",
    "\n",
    "    index = 0\n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = imgs[index]\n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            new_im.paste(im, (i, j))\n",
    "            index += 1\n",
    "\n",
    "    new_im.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# tf.datasets.fashion_mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "batch_size = 512\n",
    "# h_dim = 20\n",
    "z_dim = 10\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "# 不需要标签\n",
    "db_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "db_test = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "\n",
    "db_train = db_train.shuffle(10000).batch(batch_size)\n",
    "db_test = db_test.batch(batch_size)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encode 编码层\n",
    "        self.fc1 = keras.layers.Dense(128)\n",
    "        # 得到mean值层\n",
    "        self.fc2 = keras.layers.Dense(z_dim)\n",
    "        # 得到方差层\n",
    "        self.fc3 = keras.layers.Dense(z_dim)\n",
    "        \n",
    "        # decode 解码层\n",
    "        self.fc4 = keras.layers.Dense(128)\n",
    "        self.fc5 = keras.layers.Dense(784)\n",
    "       \n",
    "    def encode(self, x):\n",
    "        # 编码 拿到预测均值和log方差\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        \n",
    "        mu = self.fc2(h)\n",
    "        log_var = self.fc3(h)\n",
    "        \n",
    "        return mu, log_var\n",
    "    \n",
    "    def decode(self, x):\n",
    "        out = tf.nn.relu(self.fc4(x))\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        std = tf.exp(log_var * 0.5)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        # [b, 784] => [b, z_dim], [b, z_dim]\n",
    "        mu, log_var = self.encode(inputs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              multiple                  100480    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  1290      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  1408      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  101136    \n",
      "=================================================================\n",
      "Total params: 205,604\n",
      "Trainable params: 205,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VAE()\n",
    "\n",
    "model.build(input_shape=(4, 784))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 537.3427734375\n",
      "0 100 286.2420654296875\n",
      "1 0 278.1512451171875\n",
      "1 100 258.4302978515625\n",
      "2 0 254.70762634277344\n",
      "2 100 251.5272979736328\n",
      "3 0 248.7001495361328\n",
      "3 100 247.20147705078125\n",
      "4 0 244.83370971679688\n",
      "4 100 245.36611938476562\n",
      "5 0 242.1671142578125\n",
      "5 100 242.462646484375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5031d55152de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m     if (context.executing_eagerly()\n\u001b[1;32m    293\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[0;32m--> 294\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m       \u001b[0mds_variant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         dataset = _OptimizeDataset(dataset, static_optimizations,\n\u001b[0;32m--> 260\u001b[0;31m                                    static_optimization_configs)\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mautotune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations, optimization_configs)\u001b[0m\n\u001b[1;32m   3569\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3570\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3571\u001b[0;31m         **flat_structure(self))\n\u001b[0m\u001b[1;32m   3572\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset\u001b[0;34m(input_dataset, optimizations, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   3381\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3382\u001b[0m         \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimization_configs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3383\u001b[0;31m         optimization_configs)\n\u001b[0m\u001b[1;32m   3384\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3385\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "optimizer = tf.optimizers.Adam(1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for step, x in enumerate(db_train):\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, [-1, 28 * 28])\n",
    "        with tf.GradientTape() as tap:\n",
    "            x_rec_logits, mu, log_var = model(x)\n",
    "            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n",
    "            rec_loss = tf.reduce_sum(rec_loss) / x.shape[0]\n",
    "            \n",
    "            # 计算散度\n",
    "            # compute kl divergence (mu, var) ~ N (0, 1)\n",
    "            # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n",
    "            kl_div = -0.5 * (log_var + 1 - mu**2 - tf.exp(log_var))\n",
    "            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n",
    "            \n",
    "            loss = rec_loss + 1. * kl_div\n",
    "            \n",
    "\n",
    "        # 计算梯度 更新梯度\n",
    "        grads = tap.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 100 ==0:\n",
    "            print(epoch, step, float(rec_loss))\n",
    "\n",
    "        # 测试\n",
    "        # 随机生成一个正太分布\n",
    "        z = tf.random.normal((batch_size, z_dim))\n",
    "        logits = model.decode(z)\n",
    "\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        # [b, 784] => [b, 28, 28]\n",
    "#         x_hat = tf.reshape(x_hat, [-1, 28, 28])\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n",
    "\n",
    "        x_hat = x_hat.astype(np.uint8)\n",
    "        save_images(x_hat, 'vae_images/sampled_epoch%d.png'%epoch)\n",
    "\n",
    "        \n",
    "        x = next(iter(db_test))\n",
    "        x = tf.reshape(x, [-1, 28 *28])\n",
    "        logits, _, _ = model(x)\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "        # [b, 784] => [b, 28, 28]\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n",
    "\n",
    "        x_hat = x_hat.astype(np.uint8)\n",
    "        save_images(x_hat, 'vae_images/rec_epoch%d.png'%epoch)\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
