{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置项\n",
    "# # 这个要放到设置中文之前否则还是小方框\n",
    "# plt.style.use(\"seaborn\")\n",
    "\n",
    "# # 指定默认字体 用来正常显示中文标签\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# # 解决保存图像是负号'-'显示为方块的问题\n",
    "# plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# #全部行都能输出\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.map(preprocess_data).shuffle(10000).batch(128).repeat(10)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess_data).shuffle(10000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  330       \n",
      "=================================================================\n",
      "Total params: 576,810\n",
      "Trainable params: 576,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(rate=0.3), # 30%的连接节点弃用\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.3),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.build(input_shape=[None, 28 * 28])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输出日志 用于可视化\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = 'logs/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(logdir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.947890520095825 loss_regularization: 637.0550537109375\n",
      "【正确率 accuracy_metrics = 0.097400】\n",
      "100 loss: 0.5937192440032959 loss_regularization: 324.5043640136719\n",
      "200 loss: 0.4343646466732025 loss_regularization: 309.3497619628906\n",
      "300 loss: 0.48443251848220825 loss_regularization: 284.9587097167969\n",
      "400 loss: 0.45334550738334656 loss_regularization: 279.78680419921875\n",
      "500 loss: 0.45048758387565613 loss_regularization: 261.0043640136719\n",
      "【正确率 accuracy_metrics = 0.927800】\n",
      "600 loss: 0.39158862829208374 loss_regularization: 256.7874755859375\n",
      "700 loss: 0.4429070055484772 loss_regularization: 265.77374267578125\n",
      "800 loss: 0.4422701597213745 loss_regularization: 240.0529022216797\n",
      "900 loss: 0.5517773628234863 loss_regularization: 252.266845703125\n",
      "1000 loss: 0.4606044590473175 loss_regularization: 235.90267944335938\n",
      "【正确率 accuracy_metrics = 0.947500】\n",
      "1100 loss: 0.42014020681381226 loss_regularization: 236.43417358398438\n",
      "1200 loss: 0.4014856219291687 loss_regularization: 252.77291870117188\n",
      "1300 loss: 0.41373270750045776 loss_regularization: 253.0390625\n",
      "1400 loss: 0.4546980559825897 loss_regularization: 241.5715789794922\n",
      "1500 loss: 0.4841730296611786 loss_regularization: 261.4374084472656\n",
      "【正确率 accuracy_metrics = 0.944200】\n",
      "1600 loss: 0.4229981005191803 loss_regularization: 251.13168334960938\n",
      "1700 loss: 0.6276034116744995 loss_regularization: 262.3189392089844\n",
      "1800 loss: 0.4811636209487915 loss_regularization: 238.9459991455078\n",
      "1900 loss: 0.3166395127773285 loss_regularization: 226.76666259765625\n",
      "2000 loss: 0.3552161455154419 loss_regularization: 222.18153381347656\n",
      "【正确率 accuracy_metrics = 0.953200】\n",
      "2100 loss: 0.5653429627418518 loss_regularization: 287.03289794921875\n",
      "2200 loss: 0.31085023283958435 loss_regularization: 229.34893798828125\n",
      "2300 loss: 0.32195019721984863 loss_regularization: 229.6138916015625\n",
      "2400 loss: 0.45438307523727417 loss_regularization: 216.7290802001953\n",
      "2500 loss: 0.42219406366348267 loss_regularization: 233.69528198242188\n",
      "【正确率 accuracy_metrics = 0.954400】\n",
      "2600 loss: 0.4123658537864685 loss_regularization: 238.93434143066406\n",
      "2700 loss: 0.3771531283855438 loss_regularization: 256.1239318847656\n",
      "2800 loss: 0.33738377690315247 loss_regularization: 225.85765075683594\n",
      "2900 loss: 0.530655562877655 loss_regularization: 246.71421813964844\n",
      "3000 loss: 0.48728737235069275 loss_regularization: 225.7605743408203\n",
      "【正确率 accuracy_metrics = 0.944500】\n",
      "3100 loss: 0.4862527847290039 loss_regularization: 239.0103759765625\n",
      "3200 loss: 0.39783191680908203 loss_regularization: 262.8940124511719\n",
      "3300 loss: 0.3154895305633545 loss_regularization: 235.5025634765625\n",
      "3400 loss: 0.3705592155456543 loss_regularization: 233.11021423339844\n",
      "3500 loss: 0.47463616728782654 loss_regularization: 227.72146606445312\n",
      "【正确率 accuracy_metrics = 0.946600】\n",
      "3600 loss: 0.37023425102233887 loss_regularization: 263.9284973144531\n",
      "3700 loss: 0.39629003405570984 loss_regularization: 229.9983367919922\n",
      "3800 loss: 0.32675787806510925 loss_regularization: 218.03565979003906\n",
      "3900 loss: 0.5775349736213684 loss_regularization: 234.5736541748047\n",
      "4000 loss: 0.4130628705024719 loss_regularization: 269.70880126953125\n",
      "【正确率 accuracy_metrics = 0.950300】\n",
      "4100 loss: 0.38570505380630493 loss_regularization: 241.2586212158203\n",
      "4200 loss: 0.4089234471321106 loss_regularization: 227.29107666015625\n",
      "4300 loss: 0.5268355011940002 loss_regularization: 218.29473876953125\n",
      "4400 loss: 0.37905189394950867 loss_regularization: 241.9768829345703\n",
      "4500 loss: 0.3987575173377991 loss_regularization: 250.6194305419922\n",
      "【正确率 accuracy_metrics = 0.947400】\n",
      "4600 loss: 0.5213960409164429 loss_regularization: 243.18630981445312\n"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "optimizer = keras.optimizers.Adam(0.01)\n",
    "\n",
    "# 统计准确率\n",
    "accuracy_metrics = keras.metrics.Accuracy()\n",
    "\n",
    "# 开始训练\n",
    "for step, (x, y) in enumerate(db_train):\n",
    "    x = tf.reshape(x, [-1, 28 * 28])\n",
    "    with tf.GradientTape() as tap:\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        y = tf.one_hot(y, depth=10)\n",
    "        y_pred = model(x)\n",
    "        loss = keras.losses.categorical_crossentropy(y, y_pred, from_logits=True)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        # 正则化\n",
    "        loss_regularization = []\n",
    "        for p in model.trainable_variables:\n",
    "            loss_regularization.append(tf.nn.l2_loss(p))\n",
    "         \n",
    "        # l2正则化 左右参数相加求和\n",
    "        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n",
    "        \n",
    "        loss = loss + 0.001 * loss_regularization\n",
    "    \n",
    "    # 对参数球梯度 更新参数\n",
    "    grads = tap.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n",
    "    \n",
    "    if step % 500 == 0:\n",
    "        accuracy_metrics.reset_states()\n",
    "        \n",
    "        for (x_, y_) in db_test:\n",
    "            x_ = tf.reshape(x_, [-1, 28*28])\n",
    "            y_pred = model(x_)\n",
    "            y_pred = tf.argmax(y_pred, axis=1, output_type=tf.int32)\n",
    "            \n",
    "            accuracy_metrics.update_state(y_, y_pred)\n",
    "            \n",
    "        print('【正确率 accuracy_metrics = %f】' % (accuracy_metrics.result().numpy()))    \n",
    " \n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('test_accuracy', accuracy_metrics.result().numpy(), step=step)\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
