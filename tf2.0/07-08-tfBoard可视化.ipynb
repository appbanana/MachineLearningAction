{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir logs 指令启动终端 监听6006端口 并在网页上显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y):\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255\n",
    "#     x = tf.reshape(x, [-1, 28 * 28])\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据 划分训练集和测试集\n",
    "db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "db_train = db_train.map(preprocess_data).shuffle(10000).batch(128).repeat(10)\n",
    "\n",
    "db_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "db_test = db_test.map(preprocess_data).shuffle(10000).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]], shape=(128, 28, 28), dtype=float32)\n",
      "tf.Tensor(\n",
      "[7 5 3 8 6 8 2 4 7 0 7 1 6 8 0 9 5 6 3 3 6 0 6 8 7 4 4 7 2 8 6 7 9 9 9 3 8\n",
      " 9 7 0 7 8 5 1 2 2 6 2 6 7 5 4 8 8 3 7 4 7 8 9 5 9 4 9 6 3 6 2 4 2 3 6 6 7\n",
      " 1 9 0 4 9 7 1 1 6 1 1 4 2 4 9 3 2 6 0 3 6 9 8 4 2 1 2 7 1 6 1 4 4 7 0 8 9\n",
      " 4 4 9 7 5 6 0 6 2 2 9 3 1 5 3 7 5], shape=(128,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 查看第一批次数据\n",
    "for x_sample, y_sample in db_train.take(1):\n",
    "    print(x_sample)\n",
    "    print(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              multiple                  401920    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  131328    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  330       \n",
      "=================================================================\n",
      "Total params: 576,810\n",
      "Trainable params: 576,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(10) # 输出层\n",
    "])\n",
    "\n",
    "model.build(input_shape=[None, 28 * 28])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输出日志 用于可视化\n",
    "current_time = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = 'logs/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(logdir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出训练数据中的第一张图片 用于测试\n",
    "sample_img = next(iter(db_train))[0]\n",
    "# [1, 784]\n",
    "sample_img = sample_img[0]\n",
    "# 将第二个维度(784那一维) 转化为 28 * 28 * 1 因为是黑白的,只有一个颜色通道\n",
    "sample_img = tf.reshape(sample_img, [1, 28, 28, 1])\n",
    "with summary_writer.as_default():\n",
    "    tf.summary.image('Training sample:', sample_img, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "  # Save the plot to a PNG in memory.\n",
    "  buf = io.BytesIO()\n",
    "  plt.savefig(buf, format='png')\n",
    "  # Closing the figure prevents it from being displayed directly inside\n",
    "  # the notebook.\n",
    "  plt.close(figure)\n",
    "  buf.seek(0)\n",
    "  # Convert PNG buffer to TF image\n",
    "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "  # Add the batch dimension\n",
    "  image = tf.expand_dims(image, 0)\n",
    "  return image\n",
    "\n",
    "def image_grid(images):\n",
    "  \"\"\"Return a 5x5 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
    "  # Create a figure to contain the plot.\n",
    "  figure = plt.figure(figsize=(10,10))\n",
    "  for i in range(25):\n",
    "    # Start next subplot.\n",
    "    plt.subplot(5, 5, i + 1, title='name')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "  \n",
    "  return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 1.5573608875274658\n",
      "【正确率 aucc = 0.439600】\n",
      "(25, 28, 28, 1)\n",
      "100 loss: 0.6294048428535461\n",
      "200 loss: 0.444131463766098\n",
      "300 loss: 0.44052398204803467\n",
      "400 loss: 0.4169237017631531\n",
      "500 loss: 0.3385460078716278\n",
      "【正确率 aucc = 0.862100】\n",
      "(25, 28, 28, 1)\n",
      "600 loss: 0.25533097982406616\n",
      "700 loss: 0.33047640323638916\n",
      "800 loss: 0.31786808371543884\n",
      "900 loss: 0.37006673216819763\n",
      "1000 loss: 0.2759595811367035\n",
      "【正确率 aucc = 0.866600】\n",
      "(25, 28, 28, 1)\n",
      "1100 loss: 0.32472914457321167\n",
      "1200 loss: 0.28321900963783264\n",
      "1300 loss: 0.34115663170814514\n",
      "1400 loss: 0.30202150344848633\n",
      "1500 loss: 0.2683541774749756\n",
      "【正确率 aucc = 0.858600】\n",
      "(25, 28, 28, 1)\n",
      "1600 loss: 0.2492804378271103\n",
      "1700 loss: 0.35969164967536926\n",
      "1800 loss: 0.304564505815506\n",
      "1900 loss: 0.4160808324813843\n",
      "2000 loss: 0.3882664442062378\n",
      "【正确率 aucc = 0.873600】\n",
      "(25, 28, 28, 1)\n",
      "2100 loss: 0.25992709398269653\n",
      "2200 loss: 0.3613847494125366\n",
      "2300 loss: 0.31123286485671997\n",
      "2400 loss: 0.1751662939786911\n",
      "2500 loss: 0.19821418821811676\n",
      "【正确率 aucc = 0.875600】\n",
      "(25, 28, 28, 1)\n",
      "2600 loss: 0.26547086238861084\n",
      "2700 loss: 0.30679625272750854\n",
      "2800 loss: 0.17804957926273346\n",
      "2900 loss: 0.2421838492155075\n",
      "3000 loss: 0.2786095142364502\n",
      "【正确率 aucc = 0.872000】\n",
      "(25, 28, 28, 1)\n",
      "3100 loss: 0.23627996444702148\n",
      "3200 loss: 0.2554948925971985\n",
      "3300 loss: 0.17094220221042633\n",
      "3400 loss: 0.21407747268676758\n",
      "3500 loss: 0.21683645248413086\n",
      "【正确率 aucc = 0.881800】\n",
      "(25, 28, 28, 1)\n",
      "3600 loss: 0.27590978145599365\n",
      "3700 loss: 0.265632301568985\n",
      "3800 loss: 0.23268204927444458\n",
      "3900 loss: 0.2686671018600464\n",
      "4000 loss: 0.24371513724327087\n",
      "【正确率 aucc = 0.883000】\n",
      "(25, 28, 28, 1)\n",
      "4100 loss: 0.2316860556602478\n",
      "4200 loss: 0.21714548766613007\n",
      "4300 loss: 0.18155734241008759\n",
      "4400 loss: 0.22597897052764893\n",
      "4500 loss: 0.18064062297344208\n",
      "【正确率 aucc = 0.885800】\n",
      "(25, 28, 28, 1)\n",
      "4600 loss: 0.30715513229370117\n"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "\n",
    "\n",
    "for step, (x, y) in enumerate(db_train):\n",
    "   # [batch, 28, 28, 1] => [batch, 28 * 28]\n",
    "    x = tf.reshape(x, [-1, 28 * 28])\n",
    "    y = tf.one_hot(y, depth=10)\n",
    "\n",
    "    with tf.GradientTape() as tap:\n",
    "        y_pred = model(x)\n",
    "        y_true = tf.cast(y, dtype=tf.int32)\n",
    "\n",
    "        # 交叉熵\n",
    "        loss_ce = tf.reduce_mean(keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=True))\n",
    "\n",
    "    grads = tap.gradient(loss_ce, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, 'loss:', float(loss_ce))\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('train-loss', float(loss_ce), step=step)\n",
    "\n",
    "    # 测试集上测试泛化能力\n",
    "    if step % 500 == 0:\n",
    "        # 测试集上测试泛化能力\n",
    "        total_correct, total_num = 0, 0\n",
    "        for _, (x, y) in enumerate(db_test):\n",
    "            # 还得在转换一下 否则得出的结果是[128, 10] 不是[128]的格式\n",
    "            x = tf.reshape(x, (-1, 28 * 28))     \n",
    "            y_pred = model(x)\n",
    "            y_prob = tf.nn.softmax(y_pred)\n",
    "            y_pred = tf.argmax(y_prob, output_type=tf.int32, axis=1)\n",
    "\n",
    "            # 比对\n",
    "            correct = tf.equal(y, y_pred)\n",
    "            correct = tf.cast(correct, dtype=tf.int32)\n",
    "            total_correct += tf.reduce_sum(correct)\n",
    "            total_num += x.shape[0]\n",
    "            \n",
    "        print('【正确率 aucc = %f】' % (total_correct / total_num))\n",
    "        test_imgs = x_test[:25]\n",
    "        test_imgs = tf.reshape(test_imgs, [-1, 28, 28, 1])\n",
    "        print(test_imgs.shape)\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('test-aucc:', total_correct / total_num, step=step)\n",
    "            # 图片一张一张的输出\n",
    "            tf.summary.image('test-one-by-one-img', test_imgs, max_outputs=25, step=step)\n",
    "            \n",
    "            # 图片合成之后在输出 把25张小图片合成5 * 5的大图片\n",
    "            test_imgs = tf.reshape(test_imgs, [-1, 28, 28])\n",
    "            fig = image_grid(test_imgs)\n",
    "            tf.summary.image('test_imgs_25', plot_to_image(fig), step=step)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
