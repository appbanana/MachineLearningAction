{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data():\n",
    "    f = gzip.open('BP/data/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid 函数\n",
    "    \"\"\"\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    \"\"\"\n",
    "    sigmoid 倒数\n",
    "    \"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BP(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        size: eg:[784, 30, 10]\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes) - 1\n",
    "        \n",
    "        # 第一层全连接的权重 [784, 30] 第二层全连接的权重 [30, 10]\n",
    "        self.weights = [np.random.randn(ch_in, ch_out) for ch_in, ch_out in zip(sizes[:-1], sizes[1:])]\n",
    "        # 第一层[30, 1]  第二层 [10, 1]\n",
    "        self.biases = [np.random.randn(ch_out, 1) for ch_out in sizes[1:]]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [b, 784] b是批次 batch\n",
    "        \"\"\"\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            # [b, 784] @ [784, 30] => [b, 30]\n",
    "            z = np.dot(x, w) + b\n",
    "            x = sigmoid(z)\n",
    "        return x\n",
    "    \n",
    "    def back_prop(self, x, y):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        \"\"\"\n",
    "        x = x.T\n",
    "        y = y.T\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        # 前向传播\n",
    "        # 保留每一层激活层\n",
    "        activations = []\n",
    "        # 保留中间层\n",
    "        zs = []\n",
    "        activation = x\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            # [1,784]@[784, 30] +[30, 1] => [30, 1]\n",
    "            z = np.dot(activation, w) + b\n",
    "            activation = sigmoid(z)\n",
    "            \n",
    "            activations.append(activation)\n",
    "            zs.append(z)\n",
    "            \n",
    "        # 反向传播 在输出层计算梯度\n",
    "        # [1, 10]\n",
    "        delta = activations[-1] * (1 - activations[-1]) * (activations[-1] - y)\n",
    "        nabla_b[-1] = delta\n",
    "        # [10, 1]@[1, 30] => [10, 30]\n",
    "        print(activations[-2].shape, delta.shape)\n",
    "        nabla_w[-1] = np.dot(delta.T, activations[-2])\n",
    "        \n",
    "        # 计算隐藏层的梯度\n",
    "        for l in range(2, self.num_layers +1):\n",
    "            l = -l\n",
    "            z = zs[l]\n",
    "            a = activations[l]\n",
    "            # [30, 10] @ [1, 10].T => [30, 1] * [30, 1] => [30, 1]\n",
    "            delta = np.dot(self.weights[l+1], delta.T) * a * (1 - a)\n",
    "            nabla_b[l] = delta\n",
    "            # [30, 1]@[1, 784]=>[30, 784]\n",
    "            nabla_w[l] = np.dot(delta, activations[l-1])\n",
    "            \n",
    "        return nabla_w, nabla_b\n",
    "    \n",
    "    def train(self, train_data, epochs, batch_size, lr, test_data):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "        n = len(train_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(train_data)\n",
    "            mini_batchs = [train_data[k: k+batch_size] for k in range(0, n, batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batchs:\n",
    "                self.update_mini_batch(mini_batch, lr)\n",
    "            \n",
    "            if test_data:\n",
    "                print('Epoch:{0}, {1} / {2}'.format(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print('Epoch {0} complete'.format(j))\n",
    "                \n",
    "    def update_mini_batch(self, batch, lr):\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        for x, y in batch:\n",
    "            nabla_w_, nabla_b_ = self.back_prop(x, y)\n",
    "            nabla_w = [accu + curr for accu, curr in zip(nabla_w, nabla_w_)]\n",
    "            nabla_b = [accu + curr for accu, curr in zip(nabla_b, nabla_b_)]\n",
    "            \n",
    "        nabla_w = [w / len(batch) for w in nabla_w]\n",
    "        nabla_b = [b / len(batch) for b in nabla_b]\n",
    "        \n",
    "        # 更新权重\n",
    "        self.weights = [w - lr * nabla for w, nabla in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - lr * nabla for b, nabla in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        result = [(np.argmax(self.forward(x)), y) for x, y in test_data]\n",
    "        correct = sum(int(pred == y) for pred. y in result)\n",
    "        \n",
    "        return correct\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(784, 30), (30, 10)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [784, 30, 10]\n",
    "[(ch_in, ch_out) for ch_in, ch_out in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-2.05920377, -0.4449242 , -0.85939105, -0.74131419,  0.28306388,\n",
       "        -1.21354858,  0.78614475,  0.71644905,  0.34762219,  0.54261567,\n",
       "        -0.39327054,  0.58927471, -1.07808312,  0.6843646 , -1.39626958,\n",
       "         0.61744491,  0.92156831, -1.16176544, -0.15574745, -1.70828362,\n",
       "        -1.68947581,  0.1386238 , -0.33798823, -0.23666119,  0.84980593,\n",
       "        -1.65436582,  0.63983925,  0.81637841,  1.85141465, -0.07790261]),\n",
       " array([ 0.99613261, -1.94910489, -0.1733659 , -0.44361682, -0.66468349,\n",
       "         0.13487395, -0.58768285,  1.20898997, -1.63526456,  0.68139408])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [np.random.randn(ch_out) for ch_out in sizes[1:]]\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "nabla_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 (784, 1) (10, 1)\n",
      "10000 (784, 1) ()\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = load_data_wrapper()\n",
    "print(len(train_data), train_data[0][0].shape, train_data[0][1].shape)\n",
    "print(len(test_data), test_data[0][0].shape, test_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (30,10) (10,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-dc3c74a9b2c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-18828102f36f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_data, epochs, batch_size, lr, test_data)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batchs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-18828102f36f>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, batch, lr)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mnabla_w_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_b_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maccu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maccu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_b_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-18828102f36f>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# [1,784]@[784, 30] +[30, 1] => [30, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (30,10) (10,1) "
     ]
    }
   ],
   "source": [
    "model = BP([784, 30, 10])\n",
    "model.train(train_data, 1000, 10, 0.01, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-8852729f0f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*range(0, 10, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
